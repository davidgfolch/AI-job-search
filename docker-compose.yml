# Docker Compose for AI Job Search - Development Mode
# Use root/rootPass as user/password credentials

services:
  # MySQL Database
  mysql_db:
    image: mysql:9 # MySQL 9 to match existing data version
    container_name: ai-job-search-mysql
    volumes:
      - /var/lib/mysql:/var/lib/mysql # Original bind mount to preserve existing data
      - ./scripts/mysql:/docker-entrypoint-initdb.d
    command: --log_timestamps=SYSTEM
    restart: unless-stopped
    environment:
      MYSQL_ROOT_PASSWORD: rootPass
      MYSQL_DATABASE: jobs
      MYSQL_USER: user
      MYSQL_PASSWORD: userPass
      TZ: ${GLOBAL_TZ}
    ports:
      - "3306:3306"
    healthcheck:
      test: [ "CMD", "mysqladmin", "ping", "-h", "localhost" ]
      timeout: 20s
      retries: 10
      interval: 10s
    networks:
      - ai-job-search

  # Backend - FastAPI REST API
  backend:
    build:
      context: .
      dockerfile: apps/backend/Dockerfile
    container_name: ai-job-search-backend
    volumes:
      - ./apps/backend:/app/apps/backend
      - ./apps/commonlib:/app/apps/commonlib
      - ./apps/scrapper:/app/apps/scrapper
      - ./scripts:/app/scripts
      - ./.env:/app/.env
    ports:
      - "8000:8000"
    environment:
      - PYTHONUNBUFFERED=1
      - COMMONLIB_DB_HOST=mysql_db # MySQL service name for Docker networking
      - TZ=${GLOBAL_TZ}
    depends_on:
      mysql_db:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - ai-job-search
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:8000/health" ]
      interval: 30s
      timeout: 10s
      retries: 3

  # Web - React/Vite Frontend
  web:
    build:
      context: .
      dockerfile: apps/web/Dockerfile
    container_name: ai-job-search-web
    volumes:
      - ./apps/web:/app
      - /app/node_modules # Anonymous volume for node_modules
      - ./.env:/workspace/.env # Mount .env at workspace level
    ports:
      - "5173:5173"
    environment:
      - VITE_API_URL=http://localhost:8000
      - TZ=${GLOBAL_TZ}
    depends_on:
      - backend
    restart: unless-stopped
    networks:
      - ai-job-search

  # Ollama - LLM Service for AI Enrichment
  ollama:
    image: ollama/ollama:latest
    container_name: ai-job-search-ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ${OLLAMA_MODELS_PATH:-~/.ollama}:/root/.ollama # Ollama models from host (set OLLAMA_MODELS_PATH in .env for Windows)
    healthcheck:
      test: [ "CMD", "ollama", "list" ]
      interval: 1m30s
      timeout: 10s
      retries: 3
    networks:
      - ai-job-search

  # AI Enrichment - CrewAI Service
  aienrich:
    build:
      context: .
      dockerfile: apps/aiEnrich/Dockerfile
    container_name: ai-job-search-aienrich
    volumes:
      - ./apps/aiEnrich:/app/apps/aiEnrich
      - /app/apps/aiEnrich/.venv # Anonymous volume to isolate container venv from host
      - ./apps/commonlib:/app/apps/commonlib
      - ./.env:/app/.env
    environment:
      - PYTHONUNBUFFERED=1
      - COMMONLIB_DB_HOST=mysql_db
      - TZ=${GLOBAL_TZ}
      - AI_ENRICH_OLLAMA_BASE_URL=http://ollama:11434
    depends_on:
      mysql_db:
        condition: service_healthy
      ollama:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - ai-job-search

  # AI Enrichment New - Hugging Face Local Models
  aienrichnew:
    build:
      context: .
      dockerfile: apps/aiEnrichNew/Dockerfile
    container_name: ai-job-search-aienrichnew
    volumes:
      - ./apps/aiEnrichNew:/app/apps/aiEnrichNew
      - /app/apps/aiEnrichNew/.venv # Anonymous volume to isolate container venv from host
      - ./apps/commonlib:/app/apps/commonlib
      - ./.env:/app/.env
      - ${HF_HOME:-~/.cache/huggingface}:/root/.cache/huggingface # Persist HF models (set HF_HOME in .env if needed)
    environment:
      - PYTHONUNBUFFERED=1
      - COMMONLIB_DB_HOST=mysql_db # MySQL service name for Docker networking
      - TZ=${GLOBAL_TZ}
    depends_on:
      mysql_db:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - ai-job-search

  # AI CV Matcher - Local CV Matching extracted from AiEnrichNew
  aicvmatcher:
    build:
      context: .
      dockerfile: apps/aiCvMatcher/Dockerfile
    container_name: ai-job-search-aicvmatcher
    volumes:
      - ./apps/aiCvMatcher:/app/apps/aiCvMatcher
      - /app/apps/aiCvMatcher/.venv # Anonymous volume to isolate container venv from host
      - ./apps/commonlib:/app/apps/commonlib
      - ./.env:/app/.env
      - ${HF_HOME:-~/.cache/huggingface}:/root/.cache/huggingface # Persist HF models
    environment:
      - PYTHONUNBUFFERED=1
      - COMMONLIB_DB_HOST=mysql_db
      - TZ=${GLOBAL_TZ}
    depends_on:
      mysql_db:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - ai-job-search

  # AI Enrichment 3 - Fast CPU Evaluators (mDeBERTa + GLiNER)
  aienrich3:
    build:
      context: .
      dockerfile: apps/aiEnrich3/Dockerfile
    container_name: ai-job-search-aienrich3
    volumes:
      - ./apps/aiEnrich3:/app/apps/aiEnrich3
      - /app/apps/aiEnrich3/.venv # Anonymous volume to isolate container venv from host
      - ./apps/commonlib:/app/apps/commonlib
      - ./.env:/app/.env
      - ${HF_HOME:-~/.cache/huggingface}:/root/.cache/huggingface # Persist HF models
    environment:
      - PYTHONUNBUFFERED=1
      - COMMONLIB_DB_HOST=mysql_db
      - TZ=${GLOBAL_TZ}
    depends_on:
      mysql_db:
        condition: service_healthy
    restart: unless-stopped
    networks:
      - ai-job-search

  # Scrapper - Job Scraping Service
  scrapper:
    build:
      context: .
      dockerfile: apps/scrapper/Dockerfile
    container_name: ai-job-search-scrapper
    volumes:
      - ./apps/scrapper:/app/apps/scrapper
      - ./apps/commonlib:/app/apps/commonlib
      - ./.env:/app/.env
    environment:
      - PYTHONUNBUFFERED=1
      - DISPLAY=:99 # For headless Chrome
      - TZ=${GLOBAL_TZ}
    depends_on:
      mysql_db:
        condition: service_healthy
    restart: "no" # Manual start for scrapper
    networks:
      - ai-job-search
    profiles:
      - scrapper # Optional service, enable with --profile scrapper
    # Note: Scrapper runs as a batch job, not a long-running service
    # Use: docker-compose run scrapper to execute manually

networks:
  ai-job-search:
    driver: bridge

volumes:
  mysql_data:
    driver: local
